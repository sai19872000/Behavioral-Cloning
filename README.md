# Behavioral-Cloning

The goals of this project are the following:
* Build a Convolutional neural network model to simululate the human driving behaviour 
* The final model should be able to complete one lap on the simulator. The simulator has two modes 
1) Training mode and 
2) autonomous mode 
The training mode is used to collected the training data for neural network. The autonomous mode is used to test the model.
There are two tracks available. I used both the tracks to collect the data. The first track is plain with some curvy edges where as the second track is more complex with hills and very sharp edges which is hard to keep the car on the track even on the training mode. 



[//]: # (Image References)

[image1]: ./figures/simulator.png "Simulator"
[image2]: ./figures/center.png "center"
[image3]: ./figures/clr.png "clr"
[image4]: ./figures/flipimg.png "flip image"
[image5]: ./figures/jitter.png "jitter image"
[image6]: ./figuress/hist_initial.png "initial histogram"
[image7]: ./figures/hist_drop.png "droped histogram"
[image8]: ./figures/hist_aug.png "aug histograms"
[image9]: ./figures/cropped.png "cropped"
[image10]: ./figures/model_nvidia.png "model architecture"



![alt text][image1]

## Files

This project has the following files
1) model.py (this containes the python code which inclues the CNN implementation in keras)
2) model.h5 (final trained model architecture along with weights)
3) drive.py (provided by udacity to run the model in automonous mode on simulator)
4) video.mp4 (video of the simulator in automonous mode)

to run the model on the autonomous mode use the following commad 

```sh
python drive.py model.h5	
```


## Trainining images

I initially used the training data set provided by Udacity and produced good results both on tracks and then later used data set generated by myself by driving the car on the simulator on both the tracks. The training on the tracks involved

1) 3 laps of driving clock wise on track1
2) 3 laps of driving counter clock wise on track1
3) smooth driving on curves clock wise and anti clock wise
4) recovery from crubs and edges
5) 1 lap of driving on clock wise on track 2
6) 1 lap of driving on counter clock wise on track 2

The training data includes the camera images from the dash board of the car, left and right of the car along with steering angle, throttle, brake and speed

## Image augmentation 

The center image along with the steering angle looks as follows
![png][image2]

### Using left and right images
The left and right images can also be used as additional data to train the network when a correction is added/subtracted to the left/right images. 

```python
correction = 0.25
steering_c = 0.0
steering_l = steering_c+correction
steering_r = steering_c-correction

steering = (steering_c,steering_l,steering_r)
```
The center images need no correction and hence set to zero and correction value is added the left image and the subtracted from the right image. I have tested 4 different values of correction 

| correction | 0.15 | 0.2 | 0.25 | 0.3 |

and found that "0.25" works the best. The center image along with the left and right image looks as follows, the steering angle is indicated on the top of the image

![png][image3]

### Flipping the images

The image can flipped and the corresponding steering angle is multiplied by "-1.0" to generate additional data. 

```python
image = cv2.flip(image,1)
measurement = measurement*-1.0
```
The following images shows the effect of flipping

![png][image4]


### Jitter

Random brightness, shadow and shear can be applied to the images and the sheering angle is adjusted accordingly to generate more training data. After jittering the images looks as follows

![png][image5]


## Data clipping

The initial distribution of the prediction (i.e. the steering angle) of the training data is as follows

![png][image6]

This data shows that most of the data is has steering angle of zero this is not ideal for training the network in a generic track. So I have randomly droped some data in the range to close to zero and this made the distribution as follows

![png][image7]

Using this training set I performed the image augmentation described above and got the following 

![png][image8]

## Cropping and normalizing the image

Finally the image is resized to the size of (66,200) to fit the NVIDIA self driving car architecture after clipping the top 2.8th of the image to remove all the unnecessary information such as the sky, trees etc.. and the bottom 25 pixes to remove the hood images. 

```python
img_shape = image.shape
image = image[math.floor(img_shape[0]/2.8):img_shape[0]-25, 0:img_shape[1]]
image = cv2.resize(image,(new_col,new_row), interpolation=cv2.INTER_AREA)
```

The final images is looks as

![png][image9]

The images are finally normalized by keras lambda layer
```python
model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(66,200,3)))
```
## Keras generator
I initially used a keras generator but later dropped it as it was not necessary for my computer 

## Model Architecture

I have used the Nvidia's self driving car architecture as a starting point and slightly modified to get better results. Here is the architecture

![png][image10]
In this model the normalized image goes through inital convolutional layer with 24 feature maps and 5 by 5 filter followed by 2 convolutional layers each with 5 by 5 filter with 36 and 48 feature maps. After this 2 more convolutional layers each with 64 feature maps and 3 by 3 filter follow. The feature maps are now flattened followed by 4 fully connected layers with 100, 50, 10 and 1 hidden units respectively constitute the architecture.

The keras implementation is as follows

```python
from keras.layers.core import Dropout
model = Sequential()
model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(66,200,3)))
model.add(Convolution2D(24,5,5,subsample=(2,2),activation="elu",W_regularizer=l2(0.001)))
model.add(Convolution2D(36,5,5,subsample=(2,2),activation="elu",W_regularizer=l2(0.001)))
model.add(Convolution2D(48,5,5,subsample=(2,2),activation="elu",W_regularizer=l2(0.001)))
model.add(Convolution2D(64,3,3,activation="elu",W_regularizer=l2(0.001)))
model.add(Convolution2D(64,3,3,activation="elu",W_regularizer=l2(0.001)))
model.add(Flatten())
model.add(Dense(100,W_regularizer=l2(0.001)))

model.add(ELU())
model.add(Dense(50,W_regularizer=l2(0.001)))

model.add(ELU())
model.add(Dense(10,W_regularizer=l2(0.001)))
model.add(ELU())
model.add(Dense(1))

model.compile(loss = 'mse', optimizer = Adam(lr=1e-4))
```
* I have added l2 regularizer to the weight along with 2,2 subsampling to initial 3 convolutional layers to orginal architecture to prevent overfitting and for the model to able to drive in multiple tracks

* I have used ELU() activation vs relu for smoother steering angles vs relu

* The loss funtion is choosen to 'mse' since our steering angle is a continuous variable 

* I have used Adam optimizer with a learning rate of 1e-4. I tried several learning rates ranging between 1e-2 to 1e-5 and found that the 1e-4 suits the best for this problem
