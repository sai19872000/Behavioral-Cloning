# Behavioral-Cloning

The goals of this project are the following:
* Build a Convolutional neural network model to simululate the human driving behaviour 
* The final model should be able to complete one lap on the simulator. The simulator has two modes 
1) Training mode and 
2) autonomous mode 
The training mode is used to collected the training data for neural network. The autonomous mode is used to test the model.
There are two tracks available. I used both the tracks to collect the data. The first track is plain with some curvy edges where as the second track is more complex with hills and very sharp edges which is hard to keep the car on the track even on the training mode. 



[//]: # (Image References)

[image1]: ./figures/simulator.png "Simulator"
[image2]: ./figures/center.png "center"
[image3]: ./figures/clr.png "clr"
[image4]: ./figures/flipimg.png "flip image"
[image5]: ./figures/jitter.png "jitter image"
[image6]: ./figuress/hist_initial.png "initial histogram"
[image7]: ./figures/hist_drop.png "droped histogram"
[image8]: ./figures/hist_aug.png "aug histograms"
[image9]: ./figures/cropped.png "cropped"
[image10]: ./figures/model_nvidia.png "model architecture"



![alt text][image1]

## Files

This project has the following files
1) model.py (this containes the python code which inclues the CNN implementation in keras)
2) model.h5 (final trained model architecture along with weights)
3) drive.py (provided by udacity to run the model in automonous mode on simulator)
4) video.mp4 (video of the simulator in automonous mode)

to run the model on the autonomous mode use the following commad 

```sh
python drive.py model.h5	
```


## Trainining images

I initially used the training data set provided by Udacity and produced good results both on tracks and then later used data set generated by myself by driving the car on the simulator on both the tracks. The training on the tracks involved

1) 3 laps of driving clock wise on track1
2) 3 laps of driving counter clock wise on track1
3) smooth driving on curves clock wise and anti clock wise
4) recovery from crubs and edges
5) 1 lap of driving on clock wise on track 2
6) 1 lap of driving on counter clock wise on track 2

The training data includes the camera images from the dash board of the car, left and right of the car along with steering angle, throttle, brake and speed

## Image augmentation 

The center image along with the steering angle looks as follows
![png][image2]

### Using left and right images
The left and right images can also be used as additional data to train the network when a correction is added/subtracted to the left/right images. 

```python
correction = 0.25
steering_c = 0.0
steering_l = steering_c+correction
steering_r = steering_c-correction

steering = (steering_c,steering_l,steering_r)
```
The center images need no correction and hence set to zero and correction value is added the left image and the subtracted from the right image. I have tested 4 different values of correction 

| correction | 0.15 | 0.2 | 0.25 | 0.3 |

and found that "0.25" works the best. The center image along with the left and right image looks as follows, the steering angle is indicated on the top of the image

![png][image3]

### Flipping the images

The image can flipped and the corresponding steering angle is multiplied by "-1.0" to generate additional data. 

```python
image = cv2.flip(image,1)
measurement = measurement*-1.0
```
The following images shows the effect of flipping

![png][image4]


### Jitter

Random brightness, shadow and shear can be applied to the images and the sheering angle is adjusted accordingly to generate more training data. After jittering the images looks as follows

![png][image5]


## Data clipping

The initial distribution of the prediction (i.e. the steering angle) of the training data is as follows

![png][image6]

This data shows that most of the data is has steering angle of zero this is not ideal for training the network in a generic track. So I have randomly droped some data in the range to close to zero and this made the distribution as follows

![png][image7]

Using this training set I performed the image augmentation described above and got the following 

![png][image8]

## Cropping and normalizing the image

Finally the image is resized to the size of (66,200) to fit the NVIDIA self driving car architecture after clipping the top 2.8th of the image to remove all the unnecessary information such as the sky, trees etc.. and the bottom 25 pixes to remove the hood images. 

```python
img_shape = image.shape
image = image[math.floor(img_shape[0]/2.8):img_shape[0]-25, 0:img_shape[1]]
image = cv2.resize(image,(new_col,new_row), interpolation=cv2.INTER_AREA)
```

The final images is looks as

![png][image9]

The images are finally normalized by keras lambda layer
```python
model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(66,200,3)))
```

## Model Architecture

I have used the Nvidia's self driving car architecture as a starting point and slightly modified to get better results. Here is the architecture

![png][image10]

The keras implementation is as foll
